{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "527aaec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1462 entries, 0 to 1461\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   lead_source               1334 non-null   object \n",
      " 1   industry                  1328 non-null   object \n",
      " 2   number_of_courses_viewed  1462 non-null   int64  \n",
      " 3   annual_income             1281 non-null   float64\n",
      " 4   employment_status         1362 non-null   object \n",
      " 5   location                  1399 non-null   object \n",
      " 6   interaction_count         1462 non-null   int64  \n",
      " 7   lead_score                1462 non-null   float64\n",
      " 8   converted                 1462 non-null   int64  \n",
      "dtypes: float64(2), int64(3), object(4)\n",
      "memory usage: 102.9+ KB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('lead_scoring.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e18d0f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "lead_source                 128\n",
      "industry                    134\n",
      "number_of_courses_viewed      0\n",
      "annual_income               181\n",
      "employment_status           100\n",
      "location                     63\n",
      "interaction_count             0\n",
      "lead_score                    0\n",
      "converted                     0\n",
      "dtype: int64\n",
      "\n",
      "Total missing values: 606\n"
     ]
    }
   ],
   "source": [
    "# Check missing values in the dataset\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nTotal missing values:\", df.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f591ffba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['lead_source', 'industry', 'employment_status', 'location']\n",
      "Numerical columns: ['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score', 'converted']\n",
      "\n",
      "After handling missing values:\n",
      "Missing values per column:\n",
      "lead_source                 0\n",
      "industry                    0\n",
      "number_of_courses_viewed    0\n",
      "annual_income               0\n",
      "employment_status           0\n",
      "location                    0\n",
      "interaction_count           0\n",
      "lead_score                  0\n",
      "converted                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values as per instructions\n",
    "# For categorical features, replace with 'NA'\n",
    "# For numerical features, replace with 0.0\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "print(\"Categorical columns:\", list(categorical_columns))\n",
    "print(\"Numerical columns:\", list(numerical_columns))\n",
    "\n",
    "# Replace missing values\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].fillna('NA')\n",
    "\n",
    "for col in numerical_columns:\n",
    "    df[col] = df[col].fillna(0.0)\n",
    "\n",
    "print(\"\\nAfter handling missing values:\")\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a1a1818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: Most frequent observation for 'industry' column\n",
      "============================================================\n",
      "Industry value counts:\n",
      "industry\n",
      "retail           203\n",
      "finance          200\n",
      "other            198\n",
      "healthcare       187\n",
      "education        187\n",
      "technology       179\n",
      "manufacturing    174\n",
      "NA               134\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Most frequent observation (mode): retail\n",
      "Count: 203\n",
      "Percentage: 13.89%\n"
     ]
    }
   ],
   "source": [
    "# Question 1: What is the most frequent observation (mode) for the column industry?\n",
    "print(\"Question 1: Most frequent observation for 'industry' column\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get the value counts for the industry column\n",
    "industry_counts = df['industry'].value_counts()\n",
    "print(\"Industry value counts:\")\n",
    "print(industry_counts)\n",
    "print()\n",
    "\n",
    "# Get the mode (most frequent value)\n",
    "mode_industry = df['industry'].mode()[0]\n",
    "print(f\"Most frequent observation (mode): {mode_industry}\")\n",
    "print(f\"Count: {industry_counts.iloc[0]}\")\n",
    "print(f\"Percentage: {(industry_counts.iloc[0] / len(df)) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df40a066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 2: Correlation matrix for numerical features\n",
      "============================================================\n",
      "Correlation matrix:\n",
      "                          number_of_courses_viewed  annual_income  \\\n",
      "number_of_courses_viewed                  1.000000       0.009770   \n",
      "annual_income                             0.009770       1.000000   \n",
      "interaction_count                        -0.023565       0.027036   \n",
      "lead_score                               -0.004879       0.015610   \n",
      "\n",
      "                          interaction_count  lead_score  \n",
      "number_of_courses_viewed          -0.023565   -0.004879  \n",
      "annual_income                      0.027036    0.015610  \n",
      "interaction_count                  1.000000    0.009888  \n",
      "lead_score                         0.009888    1.000000  \n",
      "\n",
      "Highest correlation: 0.0270\n",
      "Between features: annual_income and interaction_count\n",
      "\n",
      "Correlations for the specified pairs:\n",
      "interaction_count and lead_score: 0.0099\n",
      "number_of_courses_viewed and lead_score: -0.0049\n",
      "number_of_courses_viewed and interaction_count: -0.0236\n",
      "annual_income and interaction_count: 0.0270\n"
     ]
    }
   ],
   "source": [
    "# Question 2: Create correlation matrix for numerical features\n",
    "print(\"Question 2: Correlation matrix for numerical features\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get numerical columns (excluding 'converted' as it's our target variable)\n",
    "numerical_features = ['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score']\n",
    "\n",
    "# Create correlation matrix\n",
    "correlation_matrix = df[numerical_features].corr()\n",
    "print(\"Correlation matrix:\")\n",
    "print(correlation_matrix)\n",
    "print()\n",
    "\n",
    "# Find the pair with the highest correlation (excluding diagonal)\n",
    "# Create a mask to exclude diagonal elements (correlation with itself = 1.0)\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool), k=1)\n",
    "correlation_values = correlation_matrix.where(mask).stack()\n",
    "\n",
    "# Find the maximum correlation\n",
    "max_corr = correlation_values.max()\n",
    "max_corr_pair = correlation_values.idxmax()\n",
    "\n",
    "print(f\"Highest correlation: {max_corr:.4f}\")\n",
    "print(f\"Between features: {max_corr_pair[0]} and {max_corr_pair[1]}\")\n",
    "print()\n",
    "\n",
    "# Show all correlations for the specified pairs\n",
    "print(\"Correlations for the specified pairs:\")\n",
    "print(f\"interaction_count and lead_score: {correlation_matrix.loc['interaction_count', 'lead_score']:.4f}\")\n",
    "print(f\"number_of_courses_viewed and lead_score: {correlation_matrix.loc['number_of_courses_viewed', 'lead_score']:.4f}\")\n",
    "print(f\"number_of_courses_viewed and interaction_count: {correlation_matrix.loc['number_of_courses_viewed', 'interaction_count']:.4f}\")\n",
    "print(f\"annual_income and interaction_count: {correlation_matrix.loc['annual_income', 'interaction_count']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2c15fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 3: Data splitting and mutual information\n",
      "============================================================\n",
      "Training set size: 877 (60.0%)\n",
      "Validation set size: 292 (20.0%)\n",
      "Test set size: 293 (20.0%)\n",
      "Total: 1462\n",
      "\n",
      "Columns in X_train: ['lead_source', 'industry', 'number_of_courses_viewed', 'annual_income', 'employment_status', 'location', 'interaction_count', 'lead_score']\n",
      "Target variable 'converted' removed: True\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train/val/test sets (60%/20%/20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the target variable\n",
    "y = df['converted']\n",
    "X = df.drop('converted', axis=1)\n",
    "\n",
    "print(\"Question 3: Data splitting and mutual information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First split: 60% train, 40% temp (which will be split into 20% val, 20% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Second split: split the 40% temp into 20% val and 20% test\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"Validation set size: {len(X_val)} ({len(X_val)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(X_test)} ({len(X_test)/len(df)*100:.1f}%)\")\n",
    "print(f\"Total: {len(X_train) + len(X_val) + len(X_test)}\")\n",
    "print()\n",
    "\n",
    "# Verify target variable is not in the dataframe\n",
    "print(\"Columns in X_train:\", list(X_train.columns))\n",
    "print(\"Target variable 'converted' removed:\", 'converted' not in X_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91fff3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['lead_source', 'industry', 'employment_status', 'location']\n",
      "\n",
      "Sample of encoded data:\n",
      "     lead_source  industry  employment_status  location\n",
      "442            4         2                  4         7\n",
      "319            4         1                  1         7\n",
      "767            4         6                  2         1\n",
      "756            4         5                  1         5\n",
      "424            1         6                  2         7\n",
      "\n",
      "Mutual Information Scores (rounded to 2 decimals):\n",
      "lead_source: 0.04\n",
      "industry: 0.03\n",
      "employment_status: 0.02\n",
      "location: 0.02\n",
      "\n",
      "Feature with highest mutual information score: lead_source\n",
      "Score: 0.04\n"
     ]
    }
   ],
   "source": [
    "# Calculate mutual information scores between y and categorical variables (FIXED VERSION)\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Get categorical columns from the training set\n",
    "categorical_columns = X_train.select_dtypes(include=['object']).columns\n",
    "print(\"Categorical columns:\", list(categorical_columns))\n",
    "print()\n",
    "\n",
    "# Encode categorical variables for mutual information calculation\n",
    "X_train_encoded = X_train.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_train_encoded[col] = le.fit_transform(X_train[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(\"Sample of encoded data:\")\n",
    "print(X_train_encoded[categorical_columns].head())\n",
    "print()\n",
    "\n",
    "# Calculate mutual information scores\n",
    "mi_scores = mutual_info_classif(X_train_encoded[categorical_columns], y_train, random_state=42)\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "mi_results = dict(zip(categorical_columns, mi_scores))\n",
    "\n",
    "print(\"Mutual Information Scores (rounded to 2 decimals):\")\n",
    "for feature, score in mi_results.items():\n",
    "    print(f\"{feature}: {round(score, 2)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Find the feature with the highest mutual information score\n",
    "max_mi_feature = max(mi_results, key=mi_results.get)\n",
    "max_mi_score = mi_results[max_mi_feature]\n",
    "\n",
    "print(f\"Feature with highest mutual information score: {max_mi_feature}\")\n",
    "print(f\"Score: {round(max_mi_score, 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ef4578c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 4: Logistic Regression with One-Hot Encoding\n",
      "============================================================\n",
      "Categorical columns: ['lead_source', 'industry', 'employment_status', 'location']\n",
      "Numerical columns: ['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score']\n",
      "\n",
      "Training set shape after encoding: (877, 27)\n",
      "Validation set shape after encoding: (292, 27)\n",
      "\n",
      "Validation accuracy: 0.7432\n",
      "Validation accuracy (rounded to 2 decimal places): 0.74\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 4: Train logistic regression with one-hot encoding\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Question 4: Logistic Regression with One-Hot Encoding\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Apply one-hot encoding to categorical variables\n",
    "# Get categorical columns\n",
    "categorical_columns = X_train.select_dtypes(include=['object']).columns\n",
    "numerical_columns = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "print(\"Categorical columns:\", list(categorical_columns))\n",
    "print(\"Numerical columns:\", list(numerical_columns))\n",
    "print()\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform categorical features on training set\n",
    "X_train_cat_encoded = ohe.fit_transform(X_train[categorical_columns])\n",
    "X_val_cat_encoded = ohe.transform(X_val[categorical_columns])\n",
    "\n",
    "# Get feature names for encoded categorical variables\n",
    "cat_feature_names = ohe.get_feature_names_out(categorical_columns)\n",
    "\n",
    "# Combine numerical and encoded categorical features\n",
    "X_train_processed = pd.DataFrame(\n",
    "    data=np.column_stack([X_train[numerical_columns].values, X_train_cat_encoded]),\n",
    "    columns=list(numerical_columns) + list(cat_feature_names)\n",
    ")\n",
    "\n",
    "X_val_processed = pd.DataFrame(\n",
    "    data=np.column_stack([X_val[numerical_columns].values, X_val_cat_encoded]),\n",
    "    columns=list(numerical_columns) + list(cat_feature_names)\n",
    ")\n",
    "\n",
    "print(f\"Training set shape after encoding: {X_train_processed.shape}\")\n",
    "print(f\"Validation set shape after encoding: {X_val_processed.shape}\")\n",
    "print()\n",
    "\n",
    "# Train logistic regression model with specified parameters\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Make predictions on validation set\n",
    "y_val_pred = model.predict(X_val_processed)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "accuracy_rounded = round(accuracy, 2)\n",
    "\n",
    "print(f\"Validation accuracy: {accuracy:.4f}\")\n",
    "print(f\"Validation accuracy (rounded to 2 decimal places): {accuracy_rounded}\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c77bc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 5: Feature Elimination\n",
      "============================================================\n",
      "1. Training baseline model with all features...\n",
      "Baseline accuracy (all features): 0.7432\n",
      "\n",
      "2. Testing each feature by removing it...\n",
      "\n",
      "Testing without 'industry':\n",
      "  Accuracy without industry: 0.7432\n",
      "  Difference: 0.0000\n",
      "\n",
      "Testing without 'employment_status':\n",
      "  Accuracy without employment_status: 0.7466\n",
      "  Difference: -0.0034\n",
      "\n",
      "Testing without 'lead_score':\n",
      "  Accuracy without lead_score: 0.7432\n",
      "  Difference: 0.0000\n",
      "\n",
      "3. Summary of differences:\n",
      "========================================\n",
      "industry: 0.0000\n",
      "employment_status: -0.0034\n",
      "lead_score: 0.0000\n",
      "\n",
      "Feature with smallest difference (least useful): industry\n",
      "Smallest difference: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 5: Feature elimination to find the least useful feature\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Question 5: Feature Elimination\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First, train the baseline model with all features (same as Q4)\n",
    "print(\"1. Training baseline model with all features...\")\n",
    "\n",
    "# Apply one-hot encoding to categorical variables (same as Q4)\n",
    "categorical_columns = X_train.select_dtypes(include=['object']).columns\n",
    "numerical_columns = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform categorical features\n",
    "X_train_cat_encoded = ohe.fit_transform(X_train[categorical_columns])\n",
    "X_val_cat_encoded = ohe.transform(X_val[categorical_columns])\n",
    "\n",
    "# Get feature names\n",
    "cat_feature_names = ohe.get_feature_names_out(categorical_columns)\n",
    "\n",
    "# Combine numerical and encoded categorical features\n",
    "X_train_all_features = pd.DataFrame(\n",
    "    data=np.column_stack([X_train[numerical_columns].values, X_train_cat_encoded]),\n",
    "    columns=list(numerical_columns) + list(cat_feature_names)\n",
    ")\n",
    "\n",
    "X_val_all_features = pd.DataFrame(\n",
    "    data=np.column_stack([X_val[numerical_columns].values, X_val_cat_encoded]),\n",
    "    columns=list(numerical_columns) + list(cat_feature_names)\n",
    ")\n",
    "\n",
    "# Train baseline model\n",
    "baseline_model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "baseline_model.fit(X_train_all_features, y_train)\n",
    "baseline_pred = baseline_model.predict(X_val_all_features)\n",
    "baseline_accuracy = accuracy_score(y_val, baseline_pred)\n",
    "\n",
    "print(f\"Baseline accuracy (all features): {baseline_accuracy:.4f}\")\n",
    "print()\n",
    "\n",
    "# Now test each feature individually by removing it\n",
    "print(\"2. Testing each feature by removing it...\")\n",
    "print()\n",
    "\n",
    "feature_differences = {}\n",
    "\n",
    "# Test removing 'industry' (categorical feature)\n",
    "print(\"Testing without 'industry':\")\n",
    "industry_features = [col for col in X_train_all_features.columns if 'industry' in col]\n",
    "X_train_no_industry = X_train_all_features.drop(columns=industry_features)\n",
    "X_val_no_industry = X_val_all_features.drop(columns=industry_features)\n",
    "\n",
    "model_no_industry = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "model_no_industry.fit(X_train_no_industry, y_train)\n",
    "pred_no_industry = model_no_industry.predict(X_val_no_industry)\n",
    "accuracy_no_industry = accuracy_score(y_val, pred_no_industry)\n",
    "\n",
    "difference_industry = baseline_accuracy - accuracy_no_industry\n",
    "feature_differences['industry'] = difference_industry\n",
    "\n",
    "print(f\"  Accuracy without industry: {accuracy_no_industry:.4f}\")\n",
    "print(f\"  Difference: {difference_industry:.4f}\")\n",
    "print()\n",
    "\n",
    "# Test removing 'employment_status' (categorical feature)\n",
    "print(\"Testing without 'employment_status':\")\n",
    "employment_features = [col for col in X_train_all_features.columns if 'employment_status' in col]\n",
    "X_train_no_employment = X_train_all_features.drop(columns=employment_features)\n",
    "X_val_no_employment = X_val_all_features.drop(columns=employment_features)\n",
    "\n",
    "model_no_employment = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "model_no_employment.fit(X_train_no_employment, y_train)\n",
    "pred_no_employment = model_no_employment.predict(X_val_no_employment)\n",
    "accuracy_no_employment = accuracy_score(y_val, pred_no_employment)\n",
    "\n",
    "difference_employment = baseline_accuracy - accuracy_no_employment\n",
    "feature_differences['employment_status'] = difference_employment\n",
    "\n",
    "print(f\"  Accuracy without employment_status: {accuracy_no_employment:.4f}\")\n",
    "print(f\"  Difference: {difference_employment:.4f}\")\n",
    "print()\n",
    "\n",
    "# Test removing 'lead_score' (numerical feature)\n",
    "print(\"Testing without 'lead_score':\")\n",
    "X_train_no_lead_score = X_train_all_features.drop(columns=['lead_score'])\n",
    "X_val_no_lead_score = X_val_all_features.drop(columns=['lead_score'])\n",
    "\n",
    "model_no_lead_score = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "model_no_lead_score.fit(X_train_no_lead_score, y_train)\n",
    "pred_no_lead_score = model_no_lead_score.predict(X_val_no_lead_score)\n",
    "accuracy_no_lead_score = accuracy_score(y_val, pred_no_lead_score)\n",
    "\n",
    "difference_lead_score = baseline_accuracy - accuracy_no_lead_score\n",
    "feature_differences['lead_score'] = difference_lead_score\n",
    "\n",
    "print(f\"  Accuracy without lead_score: {accuracy_no_lead_score:.4f}\")\n",
    "print(f\"  Difference: {difference_lead_score:.4f}\")\n",
    "print()\n",
    "\n",
    "# Find the feature with the smallest difference (least useful)\n",
    "print(\"3. Summary of differences:\")\n",
    "print(\"=\" * 40)\n",
    "for feature, diff in feature_differences.items():\n",
    "    print(f\"{feature}: {diff:.4f}\")\n",
    "\n",
    "min_diff_feature = min(feature_differences, key=lambda x: abs(feature_differences[x]))\n",
    "min_diff_value = feature_differences[min_diff_feature]\n",
    "\n",
    "print()\n",
    "print(f\"Feature with smallest difference (least useful): {min_diff_feature}\")\n",
    "print(f\"Smallest difference: {min_diff_value:.4f}\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47431722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 6: Regularized Logistic Regression\n",
      "============================================================\n",
      "1. Preparing features (same as Q4)...\n",
      "Training set shape: (877, 27)\n",
      "Validation set shape: (292, 27)\n",
      "\n",
      "2. Testing different C values...\n",
      "\n",
      "Training model with C = 0.01\n",
      "  Validation accuracy: 0.739726\n",
      "  Validation accuracy (rounded to 3 decimals): 0.74\n",
      "\n",
      "Training model with C = 0.1\n",
      "  Validation accuracy: 0.743151\n",
      "  Validation accuracy (rounded to 3 decimals): 0.743\n",
      "\n",
      "Training model with C = 1\n",
      "  Validation accuracy: 0.743151\n",
      "  Validation accuracy (rounded to 3 decimals): 0.743\n",
      "\n",
      "Training model with C = 10\n",
      "  Validation accuracy: 0.743151\n",
      "  Validation accuracy (rounded to 3 decimals): 0.743\n",
      "\n",
      "Training model with C = 100\n",
      "  Validation accuracy: 0.743151\n",
      "  Validation accuracy (rounded to 3 decimals): 0.743\n",
      "\n",
      "3. Summary of results:\n",
      "========================================\n",
      "C = 0.01: 0.74\n",
      "C = 0.1: 0.743\n",
      "C = 1: 0.743\n",
      "C = 10: 0.743\n",
      "C = 100: 0.743\n",
      "\n",
      "Best accuracy: 0.743\n",
      "C values with best accuracy: [0.1, 1, 10, 100]\n",
      "Selected C (smallest among best): 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 6: Regularized logistic regression with different C values\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Question 6: Regularized Logistic Regression\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use the same feature preparation as Q4 (all features)\n",
    "print(\"1. Preparing features (same as Q4)...\")\n",
    "\n",
    "# Apply one-hot encoding to categorical variables\n",
    "categorical_columns = X_train.select_dtypes(include=['object']).columns\n",
    "numerical_columns = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform categorical features\n",
    "X_train_cat_encoded = ohe.fit_transform(X_train[categorical_columns])\n",
    "X_val_cat_encoded = ohe.transform(X_val[categorical_columns])\n",
    "\n",
    "# Get feature names\n",
    "cat_feature_names = ohe.get_feature_names_out(categorical_columns)\n",
    "\n",
    "# Combine numerical and encoded categorical features\n",
    "X_train_processed = pd.DataFrame(\n",
    "    data=np.column_stack([X_train[numerical_columns].values, X_train_cat_encoded]),\n",
    "    columns=list(numerical_columns) + list(cat_feature_names)\n",
    ")\n",
    "\n",
    "X_val_processed = pd.DataFrame(\n",
    "    data=np.column_stack([X_val[numerical_columns].values, X_val_cat_encoded]),\n",
    "    columns=list(numerical_columns) + list(cat_feature_names)\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train_processed.shape}\")\n",
    "print(f\"Validation set shape: {X_val_processed.shape}\")\n",
    "print()\n",
    "\n",
    "# Test different C values\n",
    "print(\"2. Testing different C values...\")\n",
    "print()\n",
    "\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "results = {}\n",
    "\n",
    "for C in C_values:\n",
    "    print(f\"Training model with C = {C}\")\n",
    "    \n",
    "    # Train logistic regression with current C value\n",
    "    model = LogisticRegression(solver='liblinear', C=C, max_iter=1000, random_state=42)\n",
    "    model.fit(X_train_processed, y_train)\n",
    "    \n",
    "    # Make predictions on validation set\n",
    "    y_val_pred = model.predict(X_val_processed)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    accuracy_rounded = round(accuracy, 3)\n",
    "    \n",
    "    results[C] = accuracy_rounded\n",
    "    \n",
    "    print(f\"  Validation accuracy: {accuracy:.6f}\")\n",
    "    print(f\"  Validation accuracy (rounded to 3 decimals): {accuracy_rounded}\")\n",
    "    print()\n",
    "\n",
    "# Find the best C value\n",
    "print(\"3. Summary of results:\")\n",
    "print(\"=\" * 40)\n",
    "for C, acc in results.items():\n",
    "    print(f\"C = {C}: {acc}\")\n",
    "\n",
    "# Find the C with the best accuracy\n",
    "best_accuracy = max(results.values())\n",
    "best_C_values = [C for C, acc in results.items() if acc == best_accuracy]\n",
    "\n",
    "print()\n",
    "print(f\"Best accuracy: {best_accuracy}\")\n",
    "print(f\"C values with best accuracy: {best_C_values}\")\n",
    "\n",
    "# If multiple C values have the same best accuracy, select the smallest one\n",
    "best_C = min(best_C_values)\n",
    "\n",
    "print(f\"Selected C (smallest among best): {best_C}\")\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
