{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "032c492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8d00a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1462, 9)\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "lead_source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "industry",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "number_of_courses_viewed",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "annual_income",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "employment_status",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "location",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "interaction_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "lead_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "converted",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "3c4d3b40-23c1-43cd-894c-382fa91645d7",
       "rows": [
        [
         "0",
         "paid_ads",
         null,
         "1",
         "79450.0",
         "unemployed",
         "south_america",
         "4",
         "0.94",
         "1"
        ],
        [
         "1",
         "social_media",
         "retail",
         "1",
         "46992.0",
         "employed",
         "south_america",
         "1",
         "0.8",
         "0"
        ],
        [
         "2",
         "events",
         "healthcare",
         "5",
         "78796.0",
         "unemployed",
         "australia",
         "3",
         "0.69",
         "1"
        ],
        [
         "3",
         "paid_ads",
         "retail",
         "2",
         "83843.0",
         null,
         "australia",
         "1",
         "0.87",
         "0"
        ],
        [
         "4",
         "referral",
         "education",
         "3",
         "85012.0",
         "self_employed",
         "europe",
         "3",
         "0.62",
         "1"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_source</th>\n",
       "      <th>industry</th>\n",
       "      <th>number_of_courses_viewed</th>\n",
       "      <th>annual_income</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>location</th>\n",
       "      <th>interaction_count</th>\n",
       "      <th>lead_score</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paid_ads</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>79450.0</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>south_america</td>\n",
       "      <td>4</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>social_media</td>\n",
       "      <td>retail</td>\n",
       "      <td>1</td>\n",
       "      <td>46992.0</td>\n",
       "      <td>employed</td>\n",
       "      <td>south_america</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>events</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>5</td>\n",
       "      <td>78796.0</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>australia</td>\n",
       "      <td>3</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paid_ads</td>\n",
       "      <td>retail</td>\n",
       "      <td>2</td>\n",
       "      <td>83843.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>australia</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>referral</td>\n",
       "      <td>education</td>\n",
       "      <td>3</td>\n",
       "      <td>85012.0</td>\n",
       "      <td>self_employed</td>\n",
       "      <td>europe</td>\n",
       "      <td>3</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lead_source    industry  number_of_courses_viewed  annual_income  \\\n",
       "0      paid_ads         NaN                         1        79450.0   \n",
       "1  social_media      retail                         1        46992.0   \n",
       "2        events  healthcare                         5        78796.0   \n",
       "3      paid_ads      retail                         2        83843.0   \n",
       "4      referral   education                         3        85012.0   \n",
       "\n",
       "  employment_status       location  interaction_count  lead_score  converted  \n",
       "0        unemployed  south_america                  4        0.94          1  \n",
       "1          employed  south_america                  1        0.80          0  \n",
       "2        unemployed      australia                  3        0.69          1  \n",
       "3               NaN      australia                  1        0.87          0  \n",
       "4     self_employed         europe                  3        0.62          1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the course lead scoring dataset\n",
    "df = pd.read_csv('course_lead_scoring.csv')\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db493d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "lead_source                 128\n",
      "industry                    134\n",
      "number_of_courses_viewed      0\n",
      "annual_income               181\n",
      "employment_status           100\n",
      "location                     63\n",
      "interaction_count             0\n",
      "lead_score                    0\n",
      "converted                     0\n",
      "dtype: int64\n",
      "\n",
      "Total missing values: 606\n",
      "\n",
      "Categorical columns: ['lead_source', 'industry', 'employment_status', 'location']\n",
      "Numerical columns: ['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score', 'converted']\n",
      "\n",
      "After filling missing values:\n",
      "Missing values in each column:\n",
      "lead_source                 0\n",
      "industry                    0\n",
      "number_of_courses_viewed    0\n",
      "annual_income               0\n",
      "employment_status           0\n",
      "location                    0\n",
      "interaction_count           0\n",
      "lead_score                  0\n",
      "converted                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data preparation: Check for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Replace missing values\n",
    "# For categorical features, replace with 'NA'\n",
    "# For numerical features, replace with 0.0\n",
    "\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"\\nCategorical columns: {categorical_cols}\")\n",
    "print(f\"Numerical columns: {numerical_cols}\")\n",
    "\n",
    "# Fill missing values\n",
    "for col in categorical_cols:\n",
    "    df_clean[col] = df_clean[col].fillna('NA')\n",
    "\n",
    "for col in numerical_cols:\n",
    "    df_clean[col] = df_clean[col].fillna(0.0)\n",
    "\n",
    "print(\"\\nAfter filling missing values:\")\n",
    "print(\"Missing values in each column:\")\n",
    "print(df_clean.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afddd44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 877 (60.0%)\n",
      "Validation set size: 292 (20.0%)\n",
      "Test set size: 293 (20.0%)\n",
      "Total: 1462\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train/validation/test with 60%/20%/20% distribution\n",
    "# Use random_state=1\n",
    "\n",
    "# First split: 60% train, 40% temp (which will be split into 20% val, 20% test)\n",
    "df_train_full, df_temp = train_test_split(df_clean, test_size=0.4, random_state=1)\n",
    "\n",
    "# Second split: Split the 40% temp into 20% validation and 20% test\n",
    "df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=1)\n",
    "\n",
    "print(f\"Training set size: {len(df_train_full)} ({len(df_train_full)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"Validation set size: {len(df_val)} ({len(df_val)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(df_test)} ({len(df_test)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"Total: {len(df_train_full) + len(df_val) + len(df_test)}\")\n",
    "\n",
    "# Verify the split\n",
    "assert len(df_train_full) + len(df_val) + len(df_test) == len(df_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46b863bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC scores for each numerical variable:\n",
      "==================================================\n",
      "lead_score: AUC = 0.6111\n",
      "number_of_courses_viewed: AUC = 0.7652\n",
      "interaction_count: AUC = 0.7272\n",
      "annual_income: AUC = 0.5446\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Question 1: ROC AUC feature importance\n",
    "# For each numerical variable, use it as score (prediction) and compute AUC with y as ground truth\n",
    "# Use the training dataset for that\n",
    "# If AUC < 0.5, invert the variable by putting \"-\" in front\n",
    "\n",
    "# Define the numerical variables to evaluate\n",
    "numerical_vars = ['lead_score', 'number_of_courses_viewed', 'interaction_count', 'annual_income']\n",
    "y_train = df_train_full['converted']\n",
    "\n",
    "auc_scores = {}\n",
    "\n",
    "print(\"ROC AUC scores for each numerical variable:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for var in numerical_vars:\n",
    "    # Use the variable as prediction score\n",
    "    scores = df_train_full[var]\n",
    "    \n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(y_train, scores)\n",
    "    \n",
    "    # If AUC < 0.5, invert the variable\n",
    "    if auc < 0.5:\n",
    "        scores_inverted = -scores\n",
    "        auc_inverted = roc_auc_score(y_train, scores_inverted)\n",
    "        auc_scores[var] = auc_inverted\n",
    "        print(f\"{var}: AUC = {auc:.4f} -> Inverted AUC = {auc_inverted:.4f}\")\n",
    "    else:\n",
    "        auc_scores[var] = auc\n",
    "        print(f\"{var}: AUC = {auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8646569c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final AUC scores (after inversion if needed):\n",
      "----------------------------------------\n",
      "lead_score: 0.6111\n",
      "number_of_courses_viewed: 0.7652\n",
      "interaction_count: 0.7272\n",
      "annual_income: 0.5446\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Answer: number_of_courses_viewed has the highest AUC = 0.7652\n"
     ]
    }
   ],
   "source": [
    "# Find which variable has the highest AUC\n",
    "print(\"Final AUC scores (after inversion if needed):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for var, auc in auc_scores.items():\n",
    "    print(f\"{var}: {auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "highest_var = max(auc_scores, key=auc_scores.get)\n",
    "highest_auc = auc_scores[highest_var]\n",
    "\n",
    "print(f\"\\nAnswer: {highest_var} has the highest AUC = {highest_auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2ac4b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (877, 31)\n",
      "Validation set shape: (292, 31)\n",
      "Feature names: ['annual_income', 'employment_status=NA', 'employment_status=employed', 'employment_status=self_employed', 'employment_status=student', 'employment_status=unemployed', 'industry=NA', 'industry=education', 'industry=finance', 'industry=healthcare']...\n"
     ]
    }
   ],
   "source": [
    "# Question 2: Training the model\n",
    "# Apply one-hot-encoding using DictVectorizer and train the logistic regression\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Prepare features and target\n",
    "features = ['lead_source', 'industry', 'number_of_courses_viewed', 'annual_income', \n",
    "           'employment_status', 'location', 'interaction_count', 'lead_score']\n",
    "\n",
    "# Convert training data to dictionary format for DictVectorizer\n",
    "train_dict = df_train_full[features].to_dict(orient='records')\n",
    "val_dict = df_val[features].to_dict(orient='records')\n",
    "\n",
    "# Initialize DictVectorizer\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "X_val = dv.transform(val_dict)\n",
    "\n",
    "# Get target variables\n",
    "y_train = df_train_full['converted']\n",
    "y_val = df_val['converted']\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Feature names: {dv.feature_names_[:10]}...\")  # Show first 10 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "299aa175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.794\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression with specified parameters\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on validation set\n",
    "y_pred = model.predict_proba(X_val)[:, 1]  # Get probability of positive class\n",
    "\n",
    "# Calculate AUC on validation dataset\n",
    "val_auc = roc_auc_score(y_val, y_pred)\n",
    "\n",
    "print(f\"Validation AUC: {val_auc:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06538bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated precision and recall for 101 thresholds\n",
      "Threshold range: 0.0 to 1.0 with step 0.01\n"
     ]
    }
   ],
   "source": [
    "# Question 3: Precision and Recall\n",
    "# Evaluate the model on all thresholds from 0.0 to 1.0 with step 0.01\n",
    "# For each threshold, compute precision and recall\n",
    "# Find where precision and recall curves intersect\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Generate thresholds from 0.0 to 1.0 with step 0.01\n",
    "thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "\n",
    "# Lists to store precision and recall values\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "# Calculate precision and recall for each threshold\n",
    "for threshold in thresholds:\n",
    "    # Convert probabilities to binary predictions using threshold\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision = precision_score(y_val, y_pred_binary, zero_division=0)\n",
    "    recall = recall_score(y_val, y_pred_binary, zero_division=0)\n",
    "    \n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "print(f\"Calculated precision and recall for {len(thresholds)} thresholds\")\n",
    "print(f\"Threshold range: {thresholds[0]} to {thresholds[-1]} with step {thresholds[1] - thresholds[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a053ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intersection point:\n",
      "Threshold: 0.590\n",
      "Precision: 0.807\n",
      "Recall: 0.807\n",
      "\n",
      "Sample values around intersection:\n",
      "Threshold 0.54: Precision=0.782, Recall=0.880, Diff=0.098\n",
      "Threshold 0.55: Precision=0.789, Recall=0.875, Diff=0.086\n",
      "Threshold 0.56: Precision=0.799, Recall=0.870, Diff=0.071\n",
      "Threshold 0.57: Precision=0.807, Recall=0.849, Diff=0.042\n",
      "Threshold 0.58: Precision=0.805, Recall=0.818, Diff=0.013\n",
      "Threshold 0.59: Precision=0.807, Recall=0.807, Diff=0.000\n",
      "Threshold 0.60: Precision=0.811, Recall=0.802, Diff=0.008\n",
      "Threshold 0.61: Precision=0.820, Recall=0.781, Diff=0.038\n",
      "Threshold 0.62: Precision=0.830, Recall=0.760, Diff=0.069\n",
      "Threshold 0.63: Precision=0.822, Recall=0.724, Diff=0.099\n",
      "Threshold 0.64: Precision=0.823, Recall=0.703, Diff=0.120\n",
      "\n",
      "Closest option to 0.590: 0.545\n",
      "\n",
      "Answer: Precision and recall curves intersect at threshold 0.590\n"
     ]
    }
   ],
   "source": [
    "# Find where precision and recall curves intersect\n",
    "# Find the threshold where the difference between precision and recall is minimized\n",
    "differences = np.abs(np.array(precisions) - np.array(recalls))\n",
    "intersection_idx = np.argmin(differences)\n",
    "intersection_threshold = thresholds[intersection_idx]\n",
    "intersection_precision = precisions[intersection_idx]\n",
    "intersection_recall = recalls[intersection_idx]\n",
    "\n",
    "print(f\"\\nIntersection point:\")\n",
    "print(f\"Threshold: {intersection_threshold:.3f}\")\n",
    "print(f\"Precision: {intersection_precision:.3f}\")\n",
    "print(f\"Recall: {intersection_recall:.3f}\")\n",
    "\n",
    "# Display some sample values around the intersection point\n",
    "print(f\"\\nSample values around intersection:\")\n",
    "start_idx = max(0, intersection_idx - 5)\n",
    "end_idx = min(len(thresholds), intersection_idx + 6)\n",
    "for i in range(start_idx, end_idx):\n",
    "    print(f\"Threshold {thresholds[i]:.2f}: Precision={precisions[i]:.3f}, Recall={recalls[i]:.3f}, Diff={abs(precisions[i] - recalls[i]):.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "847ea0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum F1 score: 0.848\n",
      "Threshold at maximum F1: 0.470\n",
      "\n",
      "Sample values around maximum F1:\n",
      "Threshold 0.42: F1=0.831\n",
      "Threshold 0.43: F1=0.831\n",
      "Threshold 0.44: F1=0.836\n",
      "Threshold 0.45: F1=0.840\n",
      "Threshold 0.46: F1=0.843\n",
      "Threshold 0.47: F1=0.848\n",
      "Threshold 0.48: F1=0.846\n",
      "Threshold 0.49: F1=0.837\n",
      "Threshold 0.50: F1=0.837\n",
      "Threshold 0.51: F1=0.841\n",
      "Threshold 0.52: F1=0.834\n",
      "\n",
      "Closest option to 0.470: 0.54\n",
      "\n",
      "Answer: F1 score is maximal at threshold 0.470\n"
     ]
    }
   ],
   "source": [
    "# Question 4: F1 Score\n",
    "# Compute F1 score for all thresholds from 0.0 to 1.0 with increment 0.01\n",
    "# Find at which threshold F1 is maximal\n",
    "# F1 = 2 * P * R / (P + R) where P is precision and R is recall\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate F1 score for each threshold\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Convert probabilities to binary predictions using threshold\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_val, y_pred_binary, zero_division=0)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Find the threshold where F1 is maximal\n",
    "max_f1_idx = np.argmax(f1_scores)\n",
    "max_f1_threshold = thresholds[max_f1_idx]\n",
    "max_f1_score = f1_scores[max_f1_idx]\n",
    "\n",
    "print(f\"Maximum F1 score: {max_f1_score:.3f}\")\n",
    "print(f\"Threshold at maximum F1: {max_f1_threshold:.3f}\")\n",
    "\n",
    "# Display some sample values around the maximum F1 point\n",
    "print(f\"\\nSample values around maximum F1:\")\n",
    "start_idx = max(0, max_f1_idx - 5)\n",
    "end_idx = min(len(thresholds), max_f1_idx + 6)\n",
    "for i in range(start_idx, end_idx):\n",
    "    print(f\"Threshold {thresholds[i]:.2f}: F1={f1_scores[i]:.3f}\")\n",
    "\n",
    "# Check which option this matches\n",
    "options = [0.14, 0.34, 0.54, 0.74]\n",
    "closest_option = min(options, key=lambda x: abs(x - max_f1_threshold))\n",
    "print(f\"\\nClosest option to {max_f1_threshold:.3f}: {closest_option}\")\n",
    "\n",
    "print(f\"\\nAnswer: F1 score is maximal at threshold {max_f1_threshold:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b2b8cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross-Validation Results:\n",
      "========================================\n",
      "Fold 1: AUC = 0.8117\n",
      "Fold 2: AUC = 0.8232\n",
      "Fold 3: AUC = 0.8364\n",
      "Fold 4: AUC = 0.8392\n",
      "Fold 5: AUC = 0.8283\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Question 5: 5-Fold Cross-Validation\n",
    "# Use KFold class from Scikit-Learn to evaluate our model on 5 different folds\n",
    "# KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# Calculate standard deviation of AUC scores across different folds\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Prepare features and target for cross-validation\n",
    "# Use the same features as before\n",
    "features = ['lead_source', 'industry', 'number_of_courses_viewed', 'annual_income', \n",
    "           'employment_status', 'location', 'interaction_count', 'lead_score']\n",
    "\n",
    "# Convert full training data to dictionary format for DictVectorizer\n",
    "train_dict_full = df_train_full[features].to_dict(orient='records')\n",
    "\n",
    "# Initialize DictVectorizer and fit on full training data\n",
    "dv_cv = DictVectorizer(sparse=False)\n",
    "dv_cv.fit(train_dict_full)\n",
    "\n",
    "# Get target variable\n",
    "y_train_full = df_train_full['converted']\n",
    "\n",
    "# Lists to store AUC scores for each fold\n",
    "auc_scores_cv = []\n",
    "\n",
    "print(\"5-Fold Cross-Validation Results:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Iterate over different folds\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kf.split(df_train_full)):\n",
    "    # Split the data into train and validation for this fold\n",
    "    df_fold_train = df_train_full.iloc[train_idx]\n",
    "    df_fold_val = df_train_full.iloc[val_idx]\n",
    "    \n",
    "    # Convert to dictionary format\n",
    "    train_dict_fold = df_fold_train[features].to_dict(orient='records')\n",
    "    val_dict_fold = df_fold_val[features].to_dict(orient='records')\n",
    "    \n",
    "    # Transform using the fitted DictVectorizer\n",
    "    X_train_fold = dv_cv.transform(train_dict_fold)\n",
    "    X_val_fold = dv_cv.transform(val_dict_fold)\n",
    "    \n",
    "    # Get target variables\n",
    "    y_train_fold = df_fold_train['converted']\n",
    "    y_val_fold = df_fold_val['converted']\n",
    "    \n",
    "    # Train the model on train with specified parameters\n",
    "    model_fold = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=1)\n",
    "    model_fold.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Make predictions on validation set\n",
    "    y_pred_fold = model_fold.predict_proba(X_val_fold)[:, 1]\n",
    "    \n",
    "    # Calculate AUC on validation\n",
    "    auc_fold = roc_auc_score(y_val_fold, y_pred_fold)\n",
    "    auc_scores_cv.append(auc_fold)\n",
    "    \n",
    "    print(f\"Fold {fold_idx + 1}: AUC = {auc_fold:.4f}\")\n",
    "\n",
    "print(\"=\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76d84180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Statistics:\n",
      "Mean AUC: 0.8278\n",
      "Standard Deviation: 0.0098\n",
      "AUC Scores: ['0.8117', '0.8232', '0.8364', '0.8392', '0.8283']\n",
      "\n",
      "Closest option to 0.0098: 0.006\n",
      "\n",
      "Answer: The standard deviation of AUC scores across different folds is 0.0098\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics for cross-validation results\n",
    "mean_auc = np.mean(auc_scores_cv)\n",
    "std_auc = np.std(auc_scores_cv)\n",
    "\n",
    "print(f\"Cross-Validation Statistics:\")\n",
    "print(f\"Mean AUC: {mean_auc:.4f}\")\n",
    "print(f\"Standard Deviation: {std_auc:.4f}\")\n",
    "print(f\"AUC Scores: {[f'{score:.4f}' for score in auc_scores_cv]}\")\n",
    "\n",
    "# Check which option this matches\n",
    "options = [0.0001, 0.006, 0.06, 0.36]\n",
    "closest_option = min(options, key=lambda x: abs(x - std_auc))\n",
    "print(f\"\\nClosest option to {std_auc:.4f}: {closest_option}\")\n",
    "\n",
    "print(f\"\\nAnswer: The standard deviation of AUC scores across different folds is {std_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1c8ad85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning Results:\n",
      "==================================================\n",
      "\n",
      "Testing C = 1e-06\n",
      "------------------------------\n",
      "  Fold 1: AUC = 0.5231\n",
      "  Fold 2: AUC = 0.6092\n",
      "  Fold 3: AUC = 0.5775\n",
      "  Fold 4: AUC = 0.4885\n",
      "  Fold 5: AUC = 0.5435\n",
      "  Mean AUC: 0.548\n",
      "  Std AUC:  0.042\n",
      "\n",
      "Testing C = 0.001\n",
      "------------------------------\n",
      "  Fold 1: AUC = 0.8440\n",
      "  Fold 2: AUC = 0.8817\n",
      "  Fold 3: AUC = 0.8817\n",
      "  Fold 4: AUC = 0.8875\n",
      "  Fold 5: AUC = 0.8667\n",
      "  Mean AUC: 0.872\n",
      "  Std AUC:  0.016\n",
      "\n",
      "Testing C = 1\n",
      "------------------------------\n",
      "  Fold 1: AUC = 0.8117\n",
      "  Fold 2: AUC = 0.8232\n",
      "  Fold 3: AUC = 0.8364\n",
      "  Fold 4: AUC = 0.8392\n",
      "  Fold 5: AUC = 0.8283\n",
      "  Mean AUC: 0.828\n",
      "  Std AUC:  0.010\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Question 6: Hyperparameter Tuning\n",
    "# Use 5-Fold cross-validation to find the best parameter C\n",
    "# Iterate over C values: [0.000001, 0.001, 1]\n",
    "# Find which C leads to the best mean score\n",
    "\n",
    "# C values to test\n",
    "C_values = [0.000001, 0.001, 1]\n",
    "\n",
    "# Initialize KFold with the same parameters as previously\n",
    "kf_tuning = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Prepare features (same as before)\n",
    "features = ['lead_source', 'industry', 'number_of_courses_viewed', 'annual_income', \n",
    "           'employment_status', 'location', 'interaction_count', 'lead_score']\n",
    "\n",
    "# Convert full training data to dictionary format for DictVectorizer\n",
    "train_dict_full = df_train_full[features].to_dict(orient='records')\n",
    "\n",
    "# Initialize DictVectorizer and fit on full training data\n",
    "dv_tuning = DictVectorizer(sparse=False)\n",
    "dv_tuning.fit(train_dict_full)\n",
    "\n",
    "# Get target variable\n",
    "y_train_full = df_train_full['converted']\n",
    "\n",
    "# Dictionary to store results for each C value\n",
    "results = {}\n",
    "\n",
    "print(\"Hyperparameter Tuning Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Iterate over C values\n",
    "for C in C_values:\n",
    "    auc_scores_cv = []\n",
    "    \n",
    "    print(f\"\\nTesting C = {C}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Iterate over different folds for this C value\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf_tuning.split(df_train_full)):\n",
    "        # Split the data into train and validation for this fold\n",
    "        df_fold_train = df_train_full.iloc[train_idx]\n",
    "        df_fold_val = df_train_full.iloc[val_idx]\n",
    "        \n",
    "        # Convert to dictionary format\n",
    "        train_dict_fold = df_fold_train[features].to_dict(orient='records')\n",
    "        val_dict_fold = df_fold_val[features].to_dict(orient='records')\n",
    "        \n",
    "        # Transform using the fitted DictVectorizer\n",
    "        X_train_fold = dv_tuning.transform(train_dict_fold)\n",
    "        X_val_fold = dv_tuning.transform(val_dict_fold)\n",
    "        \n",
    "        # Get target variables\n",
    "        y_train_fold = df_fold_train['converted']\n",
    "        y_val_fold = df_fold_val['converted']\n",
    "        \n",
    "        # Train the model with current C value\n",
    "        model_fold = LogisticRegression(solver='liblinear', C=C, max_iter=1000, random_state=1)\n",
    "        model_fold.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Make predictions on validation set\n",
    "        y_pred_fold = model_fold.predict_proba(X_val_fold)[:, 1]\n",
    "        \n",
    "        # Calculate AUC on validation\n",
    "        auc_fold = roc_auc_score(y_val_fold, y_pred_fold)\n",
    "        auc_scores_cv.append(auc_fold)\n",
    "        \n",
    "        print(f\"  Fold {fold_idx + 1}: AUC = {auc_fold:.4f}\")\n",
    "    \n",
    "    # Calculate mean and std for this C value\n",
    "    mean_auc = np.mean(auc_scores_cv)\n",
    "    std_auc = np.std(auc_scores_cv)\n",
    "    \n",
    "    # Store results\n",
    "    results[C] = {\n",
    "        'mean': mean_auc,\n",
    "        'std': std_auc,\n",
    "        'scores': auc_scores_cv\n",
    "    }\n",
    "    \n",
    "    print(f\"  Mean AUC: {mean_auc:.3f}\")\n",
    "    print(f\"  Std AUC:  {std_auc:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afc83ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Results:\n",
      "------------------------------\n",
      "C = 1e-06: Mean = 0.548, Std = 0.042\n",
      "C = 0.001: Mean = 0.872, Std = 0.016\n",
      "C = 1: Mean = 0.828, Std = 0.010\n",
      "\n",
      "Finding the best C parameter:\n",
      "------------------------------\n",
      "Best mean score: 0.872\n",
      "Selected C with best mean score: 0.001\n",
      "\n",
      "Best C parameter: 0.001\n",
      "Best mean AUC: 0.872\n",
      "Best std AUC: 0.016\n",
      "\n",
      "Answer: The best C parameter is 0.001\n"
     ]
    }
   ],
   "source": [
    "# Find the best C parameter according to the criteria:\n",
    "# 1. Best mean score\n",
    "# 2. If ties, select the score with the lowest std\n",
    "# 3. If still ties, select the smallest C\n",
    "\n",
    "print(\"Summary of Results:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Display all results\n",
    "for C in C_values:\n",
    "    mean_auc = results[C]['mean']\n",
    "    std_auc = results[C]['std']\n",
    "    print(f\"C = {C}: Mean = {mean_auc:.3f}, Std = {std_auc:.3f}\")\n",
    "\n",
    "print(\"\\nFinding the best C parameter:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Find the best C based on the criteria\n",
    "best_mean = max(results[C]['mean'] for C in C_values)\n",
    "print(f\"Best mean score: {best_mean:.3f}\")\n",
    "\n",
    "# Find all C values with the best mean score\n",
    "best_mean_candidates = [C for C in C_values if abs(results[C]['mean'] - best_mean) < 1e-6]\n",
    "\n",
    "if len(best_mean_candidates) > 1:\n",
    "    print(f\"Tie in mean scores for C values: {best_mean_candidates}\")\n",
    "    \n",
    "    # Among those with best mean, find the one with lowest std\n",
    "    best_std = min(results[C]['std'] for C in best_mean_candidates)\n",
    "    print(f\"Best std among tied means: {best_std:.3f}\")\n",
    "    \n",
    "    # Find all C values with the best mean AND best std\n",
    "    best_std_candidates = [C for C in best_mean_candidates if abs(results[C]['std'] - best_std) < 1e-6]\n",
    "    \n",
    "    if len(best_std_candidates) > 1:\n",
    "        print(f\"Tie in std scores for C values: {best_std_candidates}\")\n",
    "        # Among those with best mean and best std, select the smallest C\n",
    "        best_C = min(best_std_candidates)\n",
    "        print(f\"Selecting smallest C among tied std scores: {best_C}\")\n",
    "    else:\n",
    "        best_C = best_std_candidates[0]\n",
    "        print(f\"Selected C with best mean and lowest std: {best_C}\")\n",
    "else:\n",
    "    best_C = best_mean_candidates[0]\n",
    "    print(f\"Selected C with best mean score: {best_C}\")\n",
    "\n",
    "# Display final result\n",
    "best_mean_auc = results[best_C]['mean']\n",
    "best_std_auc = results[best_C]['std']\n",
    "\n",
    "print(f\"\\nBest C parameter: {best_C}\")\n",
    "print(f\"Best mean AUC: {best_mean_auc:.3f}\")\n",
    "print(f\"Best std AUC: {best_std_auc:.3f}\")\n",
    "\n",
    "# Check which option this matches\n",
    "options = [0.000001, 0.001, 1]\n",
    "print(f\"\\nAnswer: The best C parameter is {best_C}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
